
# Optimizers in Machine Learning

This repository provides **detailed explanations** of various optimizers used in training neural networks, including their update formulas and guidelines on when to use them.

---

## Table of Contents
1. [Gradient Descent (SGD)](#1-gradient-descent-sgd)
2. [Momentum Optimizer](#2-momentum-optimizer)
3. [Nesterov Accelerated Gradient (NAG)](#3-nesterov-accelerated-gradient-nag)
4. [Adagrad Optimizer](#4-adagrad-optimizer)
5. [Adadelta Optimizer](#5-adadelta-optimizer)
6. [RMSProp Optimizer](#6-rmsprop-optimizer)
7. [Adam Optimizer](#7-adam-optimizer)
8. [Nadam Optimizer](#8-nadam-optimizer)
9. [Tips for Choosing Optimizers](#-tips-for-choosing-optimizers)

---

## 1. Gradient Descent (SGD)
- **Formula:**  
  ```
  Î¸ = Î¸ - Î· âˆ‡J(Î¸)
  ```
- **Description:**  
  Vanilla SGD updates weights using the gradient of the loss function with respect to the parameters.

---

## 2. Momentum Optimizer
- **Formula:**  
  ```
  v_t = Î³v_(t-1) + Î·âˆ‡J(Î¸)
  Î¸ = Î¸ - v_t
  ```
- **Description:**  
  Adds momentum to the updates to accelerate convergence.

---

## 3. Nesterov Accelerated Gradient (NAG)
- **Formula:**  
  ```
  v_t = Î³v_(t-1) + Î·âˆ‡J(Î¸ - Î³v_(t-1))
  Î¸ = Î¸ - v_t
  ```
- **Description:**  
  Looks ahead before applying the gradient, reducing oscillations.

---

## 4. Adagrad Optimizer
- **Formula:**  
  ```
  G_t = G_(t-1) + g_tÂ²
  Î¸ = Î¸ - (Î· / âˆš(G_t + Îµ)) g_t
  ```
- **Description:**  
  Adapts learning rate based on historical gradients.

---

## 5. Adadelta Optimizer
- **Formula:**  
  ```
  E[gÂ²]_t = ÏE[gÂ²]_(t-1) + (1 - Ï)g_tÂ²
  Î¸ = Î¸ - (Î· / âˆš(E[gÂ²]_t + Îµ)) g_t
  ```
- **Description:**  
  An extension of Adagrad that reduces aggressive decay.

---

## 6. RMSProp Optimizer
- **Formula:**  
  ```
  E[gÂ²]_t = Î³E[gÂ²]_(t-1) + (1 - Î³)g_tÂ²
  Î¸ = Î¸ - (Î· / âˆš(E[gÂ²]_t + Îµ)) g_t
  ```
- **Description:**  
  Fixes Adagrad's decaying learning rate issue.

---

## 7. Adam Optimizer
- **Formula:**  
  ```
  m_t = Î²â‚m_(t-1) + (1 - Î²â‚)g_t
  v_t = Î²â‚‚v_(t-1) + (1 - Î²â‚‚)g_tÂ²
  mÌ‚_t = m_t / (1 - Î²â‚áµ—), vÌ‚_t = v_t / (1 - Î²â‚‚áµ—)
  Î¸ = Î¸ - (Î· / âˆš(vÌ‚_t + Îµ)) mÌ‚_t
  ```
- **Description:**  
  Combines momentum and RMSProp.

---

## 8. Nadam Optimizer ğŸš€
- **Formula:**  
  ```
  m_t = Î²â‚m_(t-1) + (1 - Î²â‚)g_t
  v_t = Î²â‚‚v_(t-1) + (1 - Î²â‚‚)g_tÂ²
  Î¸ = Î¸ - (Î· / âˆš(v_t + Îµ)) (Î²â‚m_t + (1 - Î²â‚)g_t / (1 - Î²â‚áµ—))
  ```
- **Description:**  
  Adam with Nesterov momentum.

---

## Tips for Choosing Optimizers
- **SGD:** Simple and effective for large datasets.  
- **Momentum:** Helps overcome local minima efficiently.  
- **NAG:** Better than Momentum for oscillatory terrains.  
- **Adagrad:** Good for sparse data but suffers from aggressive decay.  
- **Adadelta & RMSProp:** Improve on Adagradâ€™s decay issue.  
- **Adam:** Most popular choice due to adaptive learning rates.  
- **Nadam:** Ideal for tasks needing both momentum and lookahead capabilities.

---