**Gaussian Mixture Model (GMM) & Hidden Markov Model (HMM)**
============================================================

**1️⃣ Gaussian Mixture Model (GMM)**
------------------------------------

### **Overview**

A **Gaussian Mixture Model (GMM)** is a **probabilistic model** used for
**clustering** and **density estimation**. It assumes that the data is
generated from a mixture of multiple **Gaussian distributions**.

### **Mathematical Formulation**

A GMM is defined as:

\[ P(X) = `\sum`{=tex}\_{i=1}\^{K} `\pi`{=tex}\_i `\mathcal{N}`{=tex}(X
\| `\mu`{=tex}\_i, `\Sigma`{=tex}\_i) \]

where: - ( K ) = number of Gaussian components (clusters) - (
`\pi`{=tex}*i ) = **mixing coefficient** (probability of selecting
component ( i )), with ( `\sum`{=tex}*{i=1}\^{K} `\pi`{=tex}\_i = 1 ) -
( `\mathcal{N}`{=tex}(X \| `\mu`{=tex}\_i, `\Sigma`{=tex}\_i) ) =
**multivariate normal distribution** with: - **Mean** ( `\mu`{=tex}\_i )
- **Covariance matrix** ( `\Sigma`{=tex}\_i )

### **Expectation-Maximization (EM) Algorithm**

GMM is trained using the **EM algorithm**, which iteratively updates
parameters until convergence.

#### **Step 1: Initialization**

-   Initialize means ( `\mu`{=tex}\_i ), covariances ( `\Sigma`{=tex}\_i
    ), and mixing coefficients ( `\pi`{=tex}\_i ).
-   Scikit-learn supports different initialization methods: `"kmeans"`,
    `"random"`, `"random_from_data"`.

#### **Step 2: Expectation Step (E-Step)**

-   Compute the probability (responsibilities) of each data point
    belonging to each Gaussian:

\[ r\_{i,j} =
`\frac{\pi_i \mathcal{N}(x_j | \mu_i, \Sigma_i)}{\sum_{k=1}^{K} \pi_k \mathcal{N}(x_j | \mu_k, \Sigma_k)}`{=tex}
\]

#### **Step 3: Maximization Step (M-Step)**

-   Update parameters using weighted averages:

\[ `\mu`{=tex}\_i\^{`\text{new}`{=tex}} =
`\frac{\sum_{j=1}^{N} r_{i,j} x_j}{\sum_{j=1}^{N} r_{i,j}}`{=tex} \]

\[ `\Sigma`{=tex}\_i\^{`\text{new}`{=tex}} =
`\frac{\sum_{j=1}^{N} r_{i,j} (x_j - \mu_i)(x_j - \mu_i)^T}{\sum_{j=1}^{N} r_{i,j}}`{=tex}
\]

\[ `\pi`{=tex}\_i\^{`\text{new}`{=tex}} =
`\frac{\sum_{j=1}^{N} r_{i,j}}{N}`{=tex} \]

#### **Step 4: Check for Convergence**

-   Stop when log-likelihood improvement is below a threshold.

------------------------------------------------------------------------

**2️⃣ Hidden Markov Model (HMM)**
---------------------------------

### **Overview**

A **Hidden Markov Model (HMM)** is a **probabilistic model** used to
represent **sequential data**. It assumes: 1. There is an **underlying
Markov process** with **hidden (latent) states**. 2. Each state
generates an **observable output** based on an **emission probability
distribution**.

### **Mathematical Components**

An HMM consists of: 1. **States**: ( S = { S\_1, S\_2, ..., S\_N } )
(hidden) 2. **Observations**: ( O = { o\_1, o\_2, ..., o\_T } )
(observable) 3. **Transition Probabilities**: ( A = P(S\_t \| S\_{t-1})
) 4. **Emission Probabilities**: ( B = P(O\_t \| S\_t) ) 5. **Initial
Probabilities**: ( `\pi `{=tex}= P(S\_1) )

### **Mathematical Representation**

-   **State Transition Matrix** ( A ):

\[ A\_{ij} = P(S\_t = S\_j \| S\_{t-1} = S\_i) \]

-   **Emission Matrix** ( B ):

\[ B\_{ij} = P(O\_t \| S\_t) \]

-   **Initial State Distribution** ( `\pi `{=tex}):

\[ `\pi`{=tex}\_i = P(S\_1 = S\_i) \]

### **Key HMM Algorithms**

HMMs rely on three main algorithms:

#### **1️⃣ Forward Algorithm (Probability Estimation)**

Used to compute the probability of an observation sequence:

\[ `\alpha`{=tex}\_t(j) = P(O\_1, O\_2, ..., O\_t, S\_t = S\_j) \]

#### **2️⃣ Viterbi Algorithm (Most Likely Sequence)**

Finds the most probable sequence of hidden states:

\[ `\delta`{=tex}*t(j) = `\max`{=tex}*{S\_{t-1}} P(O\_1, ..., O\_t, S\_t
= S\_j) \]

#### **3️⃣ Baum-Welch Algorithm (Parameter Learning)**

An EM-based algorithm to **train HMM parameters** from data.

------------------------------------------------------------------------

This readme.md is generated by gpt so be carefull ( i do not even cross check with my code base )
